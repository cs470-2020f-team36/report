@article{Gin2001,
   title={GIB: Imperfect Information in a Computationally Challenging Game},
   volume={14},
   ISSN={1076-9757},
   url={http://dx.doi.org/10.1613/jair.820},
   DOI={10.1613/jair.820},
   journal={Journal of Artificial Intelligence Research},
   publisher={AI Access Foundation},
   author={Ginsberg, M. L.},
   year={2001},
   month={Jun},
   pages={303–358}
}
@article{Bur2009,
author = {Buro, Michael and Long, Jeffrey and Furtak, Timothy and Sturtevant, Nathan},
year = {2009},
month = {01},
pages = {1407-1413},
title = {Improving State Evaluation, Inference, and Search in Trick-Based Card Games.}
}
@inproceedings{Stu2008,
author = {Sturtevant, Nathan R.},
title = {An Analysis of UCT in Multi-Player Games},
year = {2008},
isbn = {9783540876076},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-87608-3_4},
doi = {10.1007/978-3-540-87608-3_4},
abstract = {The UCT algorithm has been exceedingly popular for Go, a two-player game, significantly increasing the playing strength of Go programs in a very short time. This paper provides an analysis of the UCT algorithm in multi-player games, showing that UCT, when run in a multi-player game, is computing a mixed-strategy equilibrium, as opposed to maxn, which computes a pure-strategy equilibrium. We analyze the performance of UCT in several known domains and show that it performs as well or better than existing algorithms.},
booktitle = {Proceedings of the 6th International Conference on Computers and Games},
pages = {37–49},
numpages = {13},
location = {Beijing, China},
series = {CG '08}
}
@article{FraBas1998,
title = "Search in games with incomplete information: a case study using Bridge card play",
journal = "Artificial Intelligence",
volume = "100",
number = "1",
pages = "87 - 123",
year = "1998",
issn = "0004-3702",
doi = "https://doi.org/10.1016/S0004-3702(97)00082-9",
url = "http://www.sciencedirect.com/science/article/pii/S0004370297000829",
author = "Ian Frank and David Basin",
keywords = "Game tree search, Incomplete information, Game theory, Computer Bridge",
abstract = "We examine search algorithms in games with incomplete information, formalising a best defence model of such games based on the assumptions typically made when incomplete information problems are analysed in expert texts. We show that equilibrium point strategies for optimal play exist for this model, and define an algorithm capable of computing such strategies. Using this algorithm as a reference we then analyse search architectures that have been proposed for the incomplete information game of Bridge. These architectures select strategies by analysing some statistically significant collection of complete information sub-games. Our model allows us to clearly state the limitations of such architectures in producing expert analysis, and to precisely formalise and distinguish the problems that lead to sub-optimality. We illustrate these problems with simple game trees and with actual play situations from Bridge itself."
}
@article{LonStu2010,
author = {Long, Jeffrey and Sturtevant, Nathan R. and Buro, Michael and Furtak, Timothy},
title = {Understanding the Success of Perfect Information Monte Carlo Sampling in Game Tree Search},
year = {2010},
publisher = {AAAI Press},
abstract = {Perfect Information Monte Carlo (PIMC) search is a practical technique for playing imperfect information games that are too large to be optimally solved. Although PIMC search has been criticized in the past for its theoretical deficiencies, in practice it has often produced strong results in a variety of domains. In this paper, we set out to resolve this discrepancy. The contributions of the paper are twofold. First, we use synthetic game trees to identify game properties that result in strong or weak performance for PIMC search as compared to an optimal player. Second, we show how these properties can be detected in real games, and demonstrate that they do indeed appear to be good predictors of the strength of PIMC search. Thus, using the tools established in this paper, it should be possible to decide a priori whether PIMC search will be an effective approach to new and unexplored games.},
booktitle = {Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence},
pages = {134–140},
numpages = {7},
location = {Atlanta, Georgia},
series = {AAAI'10}
}
@article{KocSze2006,
author = {Kocsis, Levente and Szepesv\'{a}ri, Csaba},
title = {Bandit Based Monte-Carlo Planning},
year = {2006},
isbn = {354045375X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11871842_29},
doi = {10.1007/11871842_29},
abstract = {For large state-space Markovian Decision Problems Monte-Carlo planning is one of the few viable approaches to find near-optimal solutions. In this paper we introduce a new algorithm, UCT, that applies bandit ideas to guide Monte-Carlo planning. In finite-horizon or discounted MDPs the algorithm is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling. Experimental results show that in several domains, UCT is significantly more efficient than its alternatives.},
booktitle = {Proceedings of the 17th European Conference on Machine Learning},
pages = {282–293},
numpages = {12},
location = {Berlin, Germany},
series = {ECML'06}
}
@article{Ros2011,
author = {Rosin, Christopher},
year = {2010},
month = {09},
pages = {203-230},
title = {Multi-armed Bandits with Episode Context},
volume = {61},
journal = {Annals of Mathematics and Artificial Intelligence},
doi = {10.1007/s10472-011-9258-6}
}
@article{Sil2017,
author = {David Silver and Julian Schrittwieser and Karen Simonyan and Ioannis Antonoglou and Aja Huang and Arthur Guez and Thomas Hubert and Lucas Baker and Matthew Lai and Adrian Bolton and Yutian Chen and Timothy Lillicrap and Fan Hui and Laurent Sifre and George van den Driessche and Thore Graepel and Demis Hassabis},
year = {2017},
month = {10},
pages = {354-359},
title = {Mastering the game of Go without human knowledge},
volume = {550},
journal = {Nature},
doi = {10.1038/nature24270}
}
@manual{Ora2018,
title  = "Lessons From Implementing AlphaZero",
author = "Oracle Developers",
url    = "https://medium.com/oracledevs/lessons-from-implementing-alphazero-7e36e9054191",
year   = "2018 (visited December 13, 2020)"
}
@manual{CarOhm2019,
title  = "AlphaZero to Alpha Hero: A pre-study on Additional Tree Sampling within
Self-Play Reinforcement Learning",
author = {Fredrik Carlsson and Joey \"Ohman},
url    = "https://medium.com/oracledevs/lessons-from-implementing-alphazero-7e36e9054191",
year   = "2018 (visited December 13, 2020)"
}
@article{Mor2017,
   title={DeepStack: Expert-level artificial intelligence in heads-up no-limit poker},
   volume={356},
   ISSN={1095-9203},
   url={http://dx.doi.org/10.1126/science.aam6960},
   DOI={10.1126/science.aam6960},
   number={6337},
   journal={Science},
   publisher={American Association for the Advancement of Science (AAAS)},
   author={Moravčík, Matej and Schmid, Martin and Burch, Neil and Lisý, Viliam and Morrill, Dustin and Bard, Nolan and Davis, Trevor and Waugh, Kevin and Johanson, Michael and Bowling, Michael},
   year={2017},
   month={Mar},
   pages={508–513}
}
@article{Cow2012,
  author={P. I. {Cowling} and E. J. {Powley} and D. {Whitehouse}},
  journal={IEEE Transactions on Computational Intelligence and AI in Games}, 
  title={Information Set Monte Carlo Tree Search}, 
  year={2012},
  volume={4},
  number={2},
  pages={120-143},
  doi={10.1109/TCIAIG.2012.2200894}}
@article{CazVen2019,
      title={The ${\alpha}{\mu}$ Search Algorithm for the Game of Bridge}, 
      author={Tristan Cazenave and Véronique Ventos},
      year={2019},
      eprint={1911.07960},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@article{Bro2020,
    title={Combining Deep Reinforcement Learning and Search for Imperfect-Information Games},
    author={Noam Brown and Anton Bakhtin and Adam Lerer and Qucheng Gong},
    year={2020},
    journal={arXiv:2007.13544}
}